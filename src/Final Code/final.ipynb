{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def load(file_name):\n",
    "    df_test = pd.read_csv(file_name)\n",
    "    sLength = len(df_test['id'])\n",
    "    df_test.replace(regex={'<.*?>|&nbsp|\\W': ' '}, inplace=True)\n",
    "    df_test = df_test.fillna(\"missing\")\n",
    "    df_test['name'] = df_test['name'].str.lower()\n",
    "    df_test['lvl1'] = df_test['lvl1'].str.lower()\n",
    "    df_test['lvl2'] = df_test['lvl2'].str.lower()\n",
    "    df_test['lvl3'] = df_test['lvl3'].str.lower()\n",
    "    df_test['descrption'] = df_test['descrption'].str.lower()\n",
    "    df_test['type'] = df_test['type'].str.lower()\n",
    "    df_test['score'] = pd.Series(np.zeros(sLength, dtype=np.int), index=df_test.index)\n",
    "    return df_test\n",
    "\n",
    "def loadY(file_name):\n",
    "    df = pd.read_csv(file_name)\n",
    "    return df\n",
    "\n",
    "def loadX(file_name):\n",
    "    df_train = pd.read_csv(file_name)\n",
    "    df_label = pd.read_csv('/Users/eashan22/Desktop/DataMining/FinalProject/project/data/train_label.csv')\n",
    "    df_train = df_train.set_index('id').join(df_label.set_index('id'), how='inner')\n",
    "    df_train.replace(regex={'<.*?>|&nbsp|\\W': ' '}, inplace=True)\n",
    "    df_train['name'] = df_train['name'].str.lower()\n",
    "    df_train['lvl1'] = df_train['lvl1'].str.lower()\n",
    "    df_train['lvl2'] = df_train['lvl2'].str.lower()\n",
    "    df_train['lvl3'] = df_train['lvl3'].str.lower()\n",
    "    df_train['descrption'] = df_train['descrption'].str.lower()\n",
    "    df_train['type'] = df_train['type'].str.lower()\n",
    "    df_train = df_train.fillna(\"missing\")\n",
    "\n",
    "    # df.dropna(inplace=True)\n",
    "    # df['score'] = pd.Series(df_label['score'], index=df.index)\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['health   beautybath   bodyhand   foot careinternationalcuba heartbreaker eau de parfum spray 100ml 3 3ozformulated with oil free hydrating botanicals  remarkably improves skin texture of abused hands restores soft  smooth   refined hands'\n",
      " 'computers   laptopsstorageflash drivesinternational32gb usb 3 0 swivel flash drive memory stick storage thumb foldable pen red   interface  usb 3 0   capacity  32gb   material  abs plastic   fast data transfer rate  read  70mb sec  amp  write  30mb sec   compatible with usb 1 1 2 0   compatible with pc  notebook  mac    '\n",
      " 'mobiles   tabletsaccessoriesphone casesinternationaleican alp8l 01 metal   pc phone cover luxury aluminum frame acrylic panel hard back case for huawei p8 lite mobile phone black    aluminum frame   built in stand   shockproof   snap on off installation   push pull design easy to fit    '\n",
      " ...\n",
      " 'mobiles   tabletsaccessoriesphone casesinternationalmotomo wiredrawing pattern plastic case for iphone 6   6s 4 7  dark blue  export    designed to fit the contours of the 4 7   iphone 6     decorated withwiredrawing pattern     direct external access to all buttons controls and ports   open face design convenient to use   easy to install and remove    '\n",
      " 'fashionmenaccessoriesinternationalmetal luxury genuine cowhide leather belts automatic buck buckle business summer summer style automatic metal ripple buckle   intl   color  black   material  genuine leather   easy to use automatic buckle   belt length 125cm belt width 3 5cm   note  can be cut to fit your size     '\n",
      " 'fashionwomenbagsinternationalwomen tote messenger bag  black    intl   handbag type  totes   style  vintage   gender  for women   pattern type  solid   handbag size  small 20 30cm    closure type  cover    ']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_train =loadX('/Users/eashan22/Desktop/DataMining/FinalProject/project/data/train_data.csv')\n",
    "X_test = load('/Users/eashan22/Desktop/DataMining/FinalProject/project/data/test_data.csv')\n",
    "\n",
    "df_train = pd.DataFrame(data=X_train['lvl1']+X_train['lvl2']+X_train['lvl3']+X_train['type']+X_train['name']+X_train['descrption'],columns=['train'])\n",
    "\n",
    "X_train_text = df_train.train\n",
    "print(X_train_text.values)\n",
    "# print(df_train['lvl1'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18142,)\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.DataFrame(data=X_test['name']+X_test['lvl1']+X_test['lvl2']+X_test['lvl3']+X_test['type']+X_test['descrption'],columns=['test'])\n",
    "X_test_text = df_test.test\n",
    "# print(X_test_text.values)\n",
    "print(X_test_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer(num_words=20000)\n",
    "# tokenizer.fit_on_texts(X_train_text)\n",
    "# X_train_des = X_train.descrption\n",
    "# X_train_des = tokenizer.texts_to_sequences(X_train_des)\n",
    "# X_train_des = pad_sequences(X_train_des,maxlen=16)\n",
    "# df_train_des = pd.DataFrame(data=X_train_des)\n",
    "\n",
    "# X_train_name = X_train.name\n",
    "# X_train_name = tokenizer.texts_to_sequences(X_train_name)\n",
    "# X_train_name = pad_sequences(X_train_name, maxlen=16)\n",
    "# df_train_name = pd.DataFrame(data=X_train_name)\n",
    "\n",
    "# X_train_lvl1 = X_train.lvl1\n",
    "# X_train_lvl1 = tokenizer.texts_to_sequences(X_train_lvl1)\n",
    "# X_train_lvl1 = pad_sequences(X_train_lvl1, maxlen=4)\n",
    "# df_train_lvl1 = pd.DataFrame(data=X_train_lvl1)\n",
    "\n",
    "# X_train_lvl2 = X_train.lvl2\n",
    "# X_train_lvl2 = tokenizer.texts_to_sequences(X_train_lvl2)\n",
    "# X_train_lvl2 = pad_sequences(X_train_lvl2, maxlen=4)\n",
    "# df_train_lvl2 = pd.DataFrame(data=X_train_lvl2)\n",
    "\n",
    "# X_train_lvl3 = X_train.lvl3\n",
    "# X_train_lvl3 = tokenizer.texts_to_sequences(X_train_lvl3)\n",
    "# X_train_lvl3 = pad_sequences(X_train_lvl3, maxlen=4)\n",
    "# df_train_lvl3 = pd.DataFrame(data=X_train_lvl3)\n",
    "\n",
    "# X_train_type = X_train.type\n",
    "# X_train_type = tokenizer.texts_to_sequences(X_train_type)\n",
    "# X_train_type = pad_sequences(X_train_type, maxlen=4)\n",
    "# df_train_type = pd.DataFrame(data=X_train_type)\n",
    "\n",
    "# frames = [df_train_lvl1,df_train_lvl2,df_train_lvl3,df_train_name,df_train_des,df_train_type]\n",
    "# df_train = pd.concat(frames, axis=1)\n",
    "\n",
    "# print(df_train.values)\n",
    "# print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.DataFrame(data=X_test['name']+X_test['descrption']+X_test['lvl1']+X_test['lvl2']+X_test['lvl3']+X_test['type'],columns=['test'])\n",
    "# X_test_text = df_test.test\n",
    "# tokenizer.fit_on_texts(X_test_text)\n",
    "\n",
    "# X_test_des = X_test.descrption\n",
    "# X_test_des = tokenizer.texts_to_sequences(X_test_des)\n",
    "# X_test_des = pad_sequences(X_test_des,maxlen = 16)\n",
    "# df_test_des = pd.DataFrame(data=X_test_des)\n",
    "\n",
    "# X_test_name = X_test.name\n",
    "# X_test_name = tokenizer.texts_to_sequences(X_test_name)\n",
    "# X_test_name = pad_sequences(X_test_name,maxlen = 16)\n",
    "# df_test_name = pd.DataFrame(data=X_test_name)\n",
    "\n",
    "# X_test_lvl1 = X_test.lvl1\n",
    "# X_test_lvl1 = tokenizer.texts_to_sequences(X_test_lvl1)\n",
    "# X_test_lvl1 = pad_sequences(X_test_lvl1, maxlen=4)\n",
    "# df_test_lvl1 = pd.DataFrame(data=X_test_lvl1)\n",
    "\n",
    "# X_test_lvl2 = X_test.lvl2\n",
    "# X_test_lvl2 = tokenizer.texts_to_sequences(X_test_lvl2)\n",
    "# X_test_lvl2 = pad_sequences(X_test_lvl2, maxlen=4)\n",
    "# df_test_lvl2 = pd.DataFrame(data=X_test_lvl2)\n",
    "\n",
    "# X_test_lvl3 = X_test.lvl3\n",
    "# X_test_lvl3 = tokenizer.texts_to_sequences(X_test_lvl3)\n",
    "# X_test_lvl3 = pad_sequences(X_test_lvl3, maxlen=4)\n",
    "# df_test_lvl3 = pd.DataFrame(data=X_test_lvl3)\n",
    "\n",
    "# X_test_type = X_test.type\n",
    "# X_test_type = tokenizer.texts_to_sequences(X_test_type)\n",
    "# X_test_type = pad_sequences(X_test_type, maxlen=4)\n",
    "# df_test_type = pd.DataFrame(data=X_test_type)\n",
    "\n",
    "# frames = [df_test_lvl1,df_test_lvl2,df_test_lvl3,df_test_name,df_test_des,df_test_type]\n",
    "# df_test = pd.concat(frames,axis=1)\n",
    "\n",
    "# print(df_test.values)\n",
    "# print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...  278 2329  625]\n",
      " [   0    0    0 ...  126  637  641]\n",
      " [   0    0    0 ...   25    3  125]\n",
      " ...\n",
      " [   0    0    0 ...  440    1  405]\n",
      " [   0    0    0 ...  125   14   23]\n",
      " [   0    0    0 ...  425   35   47]]\n",
      "(18141, 243)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "X_train_hh = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_train_hh = pad_sequences(X_train_hh,maxlen=243)\n",
    "print(X_train_hh)\n",
    "print(X_train_hh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    5 2238 4120]\n",
      " [   0    0    0 ...   73  326 1181]\n",
      " [   0    0    0 ...   45   14  388]\n",
      " ...\n",
      " [   0    0    0 ...  812  589 3544]\n",
      " [   0    0    0 ... 1971    1 3871]\n",
      " [   0    0    0 ...   29 1976  956]]\n",
      "(18142, 243)\n"
     ]
    }
   ],
   "source": [
    "X_test_hh = tokenizer.texts_to_sequences(X_test_text)\n",
    "X_test_hh = pad_sequences(X_test_hh,maxlen=243)\n",
    "print(X_test_hh)\n",
    "print(X_test_hh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 0 1]\n",
      "(18141,)\n"
     ]
    }
   ],
   "source": [
    "y_score = X_train.score.values\n",
    "print(y_score)\n",
    "print(y_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16140 samples, validate on 2000 samples\n",
      "Epoch 1/13\n",
      "16140/16140 [==============================] - 5s 327us/step - loss: 0.6528 - acc: 0.6866 - val_loss: 0.6279 - val_acc: 0.6755\n",
      "Epoch 2/13\n",
      "16140/16140 [==============================] - 0s 31us/step - loss: 0.6192 - acc: 0.6897 - val_loss: 0.6217 - val_acc: 0.6755\n",
      "Epoch 3/13\n",
      "16140/16140 [==============================] - 0s 30us/step - loss: 0.6129 - acc: 0.6897 - val_loss: 0.6117 - val_acc: 0.6750\n",
      "Epoch 4/13\n",
      "16140/16140 [==============================] - 0s 31us/step - loss: 0.5970 - acc: 0.6928 - val_loss: 0.5892 - val_acc: 0.6780\n",
      "Epoch 5/13\n",
      "16140/16140 [==============================] - 0s 31us/step - loss: 0.5654 - acc: 0.7133 - val_loss: 0.5582 - val_acc: 0.7200\n",
      "Epoch 6/13\n",
      "16140/16140 [==============================] - 1s 31us/step - loss: 0.5326 - acc: 0.7454 - val_loss: 0.5327 - val_acc: 0.7240\n",
      "Epoch 7/13\n",
      "16140/16140 [==============================] - 1s 31us/step - loss: 0.5059 - acc: 0.7574 - val_loss: 0.5149 - val_acc: 0.7525\n",
      "Epoch 8/13\n",
      "16140/16140 [==============================] - 1s 31us/step - loss: 0.4863 - acc: 0.7734 - val_loss: 0.5041 - val_acc: 0.7590\n",
      "Epoch 9/13\n",
      "16140/16140 [==============================] - 1s 31us/step - loss: 0.4712 - acc: 0.7822 - val_loss: 0.4968 - val_acc: 0.7620\n",
      "Epoch 10/13\n",
      "16140/16140 [==============================] - 1s 31us/step - loss: 0.4545 - acc: 0.7948 - val_loss: 0.4900 - val_acc: 0.7670\n",
      "Epoch 11/13\n",
      "16140/16140 [==============================] - 0s 31us/step - loss: 0.4401 - acc: 0.8062 - val_loss: 0.4857 - val_acc: 0.7685\n",
      "Epoch 12/13\n",
      "16140/16140 [==============================] - 1s 32us/step - loss: 0.4254 - acc: 0.8107 - val_loss: 0.4811 - val_acc: 0.7675\n",
      "Epoch 13/13\n",
      "16140/16140 [==============================] - 1s 33us/step - loss: 0.4109 - acc: 0.8175 - val_loss: 0.4787 - val_acc: 0.7745\n",
      "[[0.87404424]\n",
      " [0.7916757 ]\n",
      " [0.514422  ]\n",
      " ...\n",
      " [0.82322294]\n",
      " [0.38494998]\n",
      " [0.8688215 ]]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(20000, 18))\n",
    "\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(30, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dropout(rate=0.5))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(X_train_hh[2001:],y_score[2001:],\n",
    "          epochs=13,\n",
    "          batch_size=192,\n",
    "          validation_data=(X_train_hh[0:2000], X_train[0:2000].score.values),\n",
    "          verbose=1)\n",
    "\n",
    "output_array = model.predict(X_test_hh)\n",
    "\n",
    "\n",
    "print(output_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_end_id = pd.DataFrame(data=X_test.id)\n",
    "# print(X_test.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id     score\n",
      "0      18142  0.903027\n",
      "1      18143  0.867133\n",
      "2      18144  0.528902\n",
      "3      18145  0.943663\n",
      "4      18146  0.910467\n",
      "5      18147  0.844827\n",
      "6      18148  0.240060\n",
      "7      18149  0.913365\n",
      "8      18150  0.865241\n",
      "9      18151  0.772387\n",
      "10     18152  0.910509\n",
      "11     18153  0.360984\n",
      "12     18154  0.955943\n",
      "13     18155  0.588503\n",
      "14     18156  0.932398\n",
      "15     18157  0.127000\n",
      "16     18158  0.814373\n",
      "17     18159  0.483881\n",
      "18     18160  0.921859\n",
      "19     18161  0.396744\n",
      "20     18162  0.994675\n",
      "21     18163  0.609777\n",
      "22     18164  0.934683\n",
      "23     18165  0.817683\n",
      "24     18166  0.958504\n",
      "25     18167  0.755571\n",
      "26     18168  0.867065\n",
      "27     18169  0.937922\n",
      "28     18170  0.680023\n",
      "29     18171  0.970930\n",
      "...      ...       ...\n",
      "18112  36254  0.918440\n",
      "18113  36255  0.541725\n",
      "18114  36256  0.929651\n",
      "18115  36257  0.713764\n",
      "18116  36258  0.202769\n",
      "18117  36259  0.196672\n",
      "18118  36260  0.852211\n",
      "18119  36261  0.064765\n",
      "18120  36262  0.550650\n",
      "18121  36263  0.783211\n",
      "18122  36264  0.108567\n",
      "18123  36265  0.237046\n",
      "18124  36266  0.902663\n",
      "18125  36267  0.490319\n",
      "18126  36268  0.968819\n",
      "18127  36269  0.840995\n",
      "18128  36270  0.944626\n",
      "18129  36271  0.845860\n",
      "18130  36272  0.225531\n",
      "18131  36273  0.253464\n",
      "18132  36274  0.897986\n",
      "18133  36275  0.910458\n",
      "18134  36276  0.845283\n",
      "18135  36277  0.829500\n",
      "18136  36278  0.468016\n",
      "18137  36279  0.978019\n",
      "18138  36280  0.339375\n",
      "18139  36281  0.874351\n",
      "18140  36282  0.299435\n",
      "18141  36283  0.911140\n",
      "\n",
      "[18142 rows x 2 columns]\n",
      "(18142, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_end_score = pd.DataFrame(data=output_array,columns=['score'])\n",
    "df_frames = [df_end_id,df_end_score]\n",
    "df_end = pd.concat(df_frames,axis=1)\n",
    "\n",
    "df_end.to_csv('/Users/eashan22/Desktop/DataMining/FinalProject/project/data/output7.csv',index=False)\n",
    "print(df_end)\n",
    "print(df_end.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
