{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/Users/eashan22/Desktop/DataMining/FinalProject/Dataset/train_data.csv')\n",
    "train_labels = pd.read_csv('/Users/eashan22/Desktop/DataMining/FinalProject/Dataset/train_label.csv')\n",
    "test_data = pd.read_csv('/Users/eashan22/Desktop/DataMining/FinalProject/Dataset/test_data.csv')\n",
    "\n",
    "total_data = train_data.append(test_data).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>lvl1</th>\n",
       "      <th>lvl2</th>\n",
       "      <th>lvl3</th>\n",
       "      <th>descrption</th>\n",
       "      <th>price</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Cuba Heartbreaker Eau De Parfum Spray 100ml/3.3oz</td>\n",
       "      <td>Health &amp; Beauty</td>\n",
       "      <td>Bath &amp; Body</td>\n",
       "      <td>Hand &amp; Foot Care</td>\n",
       "      <td>Formulated with oil-free hydrating botanicals/...</td>\n",
       "      <td>128.0</td>\n",
       "      <td>international</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               name             lvl1  \\\n",
       "0   1  Cuba Heartbreaker Eau De Parfum Spray 100ml/3.3oz  Health & Beauty   \n",
       "\n",
       "          lvl2              lvl3  \\\n",
       "0  Bath & Body  Hand & Foot Care   \n",
       "\n",
       "                                          descrption  price           type  \n",
       "0  Formulated with oil-free hydrating botanicals/...  128.0  international  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36283, 8) (36283, 8)\n",
      "id               0\n",
      "name             0\n",
      "lvl1             0\n",
      "lvl2             0\n",
      "lvl3          2135\n",
      "descrption      33\n",
      "price            0\n",
      "type           277\n",
      "dtype: int64\n",
      "id               0\n",
      "name             0\n",
      "lvl1             0\n",
      "lvl2             0\n",
      "lvl3          2135\n",
      "descrption      33\n",
      "price            0\n",
      "type           277\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(total_data.shape,total_data.shape)\n",
    "print(total_data.isnull().sum())\n",
    "print(total_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lvl1 = total_data.lvl1.unique()\n",
    "# dic={}\n",
    "# for i,lvl1 in enumerate(lvl1):\n",
    "#     if lvl1 not in dic:\n",
    "#         dic[lvl1]=i\n",
    "# labels=total_data.lvl1.apply(lambda x:dic[x])\n",
    "\n",
    "# total_data.lvl1 = labels\n",
    "# total_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lvl2 = total_data.lvl2.unique()\n",
    "# dic={}\n",
    "# for i,lvl2 in enumerate(lvl2):\n",
    "#     if lvl2 not in dic:\n",
    "#         dic[lvl2]=i+9\n",
    "# labels=total_data.lvl2.apply(lambda x:dic[x])\n",
    "\n",
    "# total_data.lvl2 = labels\n",
    "#labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lvl3 = total_data.lvl3.unique()\n",
    "# dic={}\n",
    "# for i,lvl3 in enumerate(lvl3):\n",
    "#     if lvl3 not in dic:\n",
    "#         dic[lvl3]=i+66\n",
    "# labels=total_data.lvl3.apply(lambda x:dic[x])\n",
    "\n",
    "# total_data.lvl3 = labels\n",
    "# total_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = total_data.replace(np.nan, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_data['type'] = total_data['type'].replace('international', 1)\n",
    "# total_data['type'] = total_data['type'].replace('local', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_data=total_data.sample(frac=0.2,random_state=200)\n",
    "# total_data=total_data.drop(val_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into 50% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# texts = X_train[['name','descrption']]\n",
    "# texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuba Heartbreaker Eau De Parfum Spray 100ml/3.3oz\n",
      "Formulated with oil-free hydrating botanicals/ Remarkably improves skin texture of abused hands/Restores soft, smooth & refined hands\n",
      "0        Cuba Heartbreaker Eau De Parfum Spray 100ml/3....\n",
      "1        32GB USB 3.0 Swivel Flash Drive Memory Stick S...\n",
      "2        Eican ALP8L-01 Metal + PC Phone Cover Luxury A...\n",
      "3        IPAKY Hybrid Luxury Silicone Case Cover And Pl...\n",
      "4        Phone case for iPhone 5/5s/SE You Can Be Sore ...\n",
      "5        NG-40C Ring-Shaped 40W 3166lm 5400K Macro Phot...\n",
      "6        Asus TP300LJ-DW004H Transformer Book Flip 4GB ...\n",
      "7        High Quality Genuine Leather Belts For Men Rev...\n",
      "8        McDonald's Coke Can Glass Limited Edition 12oz...\n",
      "9        ELENXS Stainless Steel Tea Ball Strainer Mesh ...\n",
      "10       7mm Natural Prehnite Crystal Bracelet(Green)<u...\n",
      "11       Style Girl Storage Box (Blue)(Export)<div> <ul...\n",
      "12       10x3mm Rare Earth Magnets 3mm Hole Size With P...\n",
      "13       Feelontop Punk Rock Rhinestone Star Shape Long...\n",
      "14       Foltene Eyelash &amp; Eyebrow Treatment<ul> <l...\n",
      "15       Reed Diffuser A1 (Orange)Smokeless<br> Flame l...\n",
      "16       JinGle Winner Mens women Skeleton Rose Gold St...\n",
      "17       Tecno TH989-TC3 Slim Hood (Silver/Black)<ul> <...\n",
      "18       3 Pieces Set of Stainless Steel Dental Mouth M...\n",
      "19       Joop Le Bain Soft Moments Eau De Parfum Spray ...\n",
      "20       Turbo Italia 90cm Conventional Cooker Hood T90...\n",
      "21       Juju Milky Emerald Eye Rhinestone Cheetah Stud...\n",
      "22       5 units of Korea Soothing &amp; Moisture 99% A...\n",
      "23       ZUNCLE Fashion Simple PU Leather Strap Couple ...\n",
      "24       Kdk 16\" Wall Fan (Remote) &nbsp;Oscillation Co...\n",
      "25       Asus A32-N55 Series 6 Cells Laptop Battery<ul>...\n",
      "26       ASUS Zenfone 5 9H TEMPERED - Fingerprint Resis...\n",
      "27       Morgan Garment Steamer MSI-GA220B (1500w)<ul> ...\n",
      "28       Sony VAIO PCG-481N BattPit Laptop AC Adapter C...\n",
      "29       Daniel Wellington 0207DW Classic ST Andrews Si...\n",
      "                               ...                        \n",
      "36253    11KW 220V Electric Water Heater Thermostat For...\n",
      "36254    Women Luxury Fashion Business Faux Leather Rom...\n",
      "36255    5.8G 3 Channel Video Switcher Module 3 Way Vid...\n",
      "36256    English Keyboard Matte Sticker Large White Let...\n",
      "36257    Remax 1m Dual Heads Micro-USB Lighting Mobile ...\n",
      "36258    Fashion Protective Stand Wallet Purse Credit C...\n",
      "36259    Leather Flip Case For Nokia N550 Shockproof An...\n",
      "36260    Fashion Baby Girls Black Short Sleeve T-shirt ...\n",
      "36261    Linen/Cotton Fabric 13 Pockets Wall Door Close...\n",
      "36262    UNI-T UT353 Mini LCD Display Digital Sound Lev...\n",
      "36263    sqamin CAGARNY DZ style leisure sports brand q...\n",
      "36264    7A Kinky Curly Virgin Hair 3Bundles lot 20‚Äù Hu...\n",
      "36265    PROCARE #UAT Upper Arm &amp; Thigh Slimmer 4mm...\n",
      "36266    Wrist Strap for Xiaomi Mi Band (Pink)<ul> <li>...\n",
      "36267    Appliance Galore Mobile Phone Holder 5600mAh P...\n",
      "36268    Ultra Clear Slim Case with Flip Cover for Sams...\n",
      "36269    20000mAh Wallet Style Power Bank (Black)<ul> <...\n",
      "36270    W Hair WH3700FI French Collection Lightweight ...\n",
      "36271    BUYINCOINS Professional Barber Hair Brush Spon...\n",
      "36272    ETOP Money Clips Men Wallet Multifunctional Co...\n",
      "36273    Kiehl's Creamy Eye Treatment with Avocado (Int...\n",
      "36274    Peranakan Rectangle Oven Pot with Handles<ul> ...\n",
      "36275    niceEshop Travel Series Lovely Cat and Birdcag...\n",
      "36276    niceEshop Home Digital Arm Blood Pressure Moni...\n",
      "36277    niceEshop Stainless Steel Potato Spiral Slicer...\n",
      "36278    Charger for Lenovo Flex 2 U430 S210(Export)- I...\n",
      "36279    1000 Rose Petals Wedding Decorations, dark red...\n",
      "36280    Practical Cross Type 2CH 433MHz Remote Control...\n",
      "36281    Fashionable Quartz Wrist Watch with Analog Dis...\n",
      "36282    Linear Belt Couple Watch - (9366-1)(Export) - ...\n",
      "Length: 36283, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X.name[0])\n",
    "print(X.descrption[0])\n",
    "\n",
    "X = X.name + X.descrption #+ X.type + X.lvl1 + X.lvl2 + X.lvl3\n",
    "# X.replace(regex={'<.*?>|&nbsp|\\W|':' '}, inplace=True)\n",
    "X.shape\n",
    "print(X)\n",
    "# print(X.type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[1] = X[1].replace('0','')\n",
    "# X[1] = X[1].replace('1','')\n",
    "# X[1] = X[1].replace('2','')\n",
    "# X[1] = X[1].replace('3','')\n",
    "# X[1] = X[1].replace('4','')\n",
    "# X[1] = X[1].replace('5','')\n",
    "# X[1] = X[1].replace('6','')\n",
    "# X[1] = X[1].replace('7','')\n",
    "# X[1] = X[1].replace('8','')\n",
    "# X[1] = X[1].replace('9','')\n",
    "# X[1] = str(X[1]).replace('<ul>','')\n",
    "# X[1] = X[1].replace('<li>','')\n",
    "# X[1] = X[1].replace('<div>','')\n",
    "\n",
    "# X[1] = X[1].replace('</ul>','')\n",
    "# X[1] = X[1].replace('</li>','')\n",
    "# X[1] = X[1].replace('</div>','')\n",
    "\n",
    "# X[1] = X[1].replace('<','')\n",
    "# X[1] = X[1].replace('<','')\n",
    "# X[1] = X[1].replace('<','')\n",
    "\n",
    "# X[1] = X[1].replace('>','')\n",
    "# X[1] = X[1].replace('>','')\n",
    "# X[1] = X[1].replace('>','')\n",
    "\n",
    "# X[1] = X[1].replace('%','')\n",
    "# X[1] = X[1].replace(':','')\n",
    "# X[1] = X[1].replace(',','')\n",
    "# X[1] = X[1].replace('.','')\n",
    "# X[1] = X[1].replace('/','')\n",
    "# X[1] = X[1].replace('&','')\n",
    "# X[1] = X[1].replace(';','')\n",
    "# X[1] = X[1].replace('\"','')\n",
    "\n",
    "# X[1] = X[1].replace('$','')\n",
    "# X[1] = X[1].replace('+','')\n",
    "# X[1] = X[1].replace('=','')\n",
    "# X[1] = X[1].replace('~','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36283,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X[2] = X[2].replace('0','')\n",
    "# X[2] = X[2].replace('1','')\n",
    "# X[2] = X[2].replace('2','')\n",
    "# X[2] = X[2].replace('3','')\n",
    "# X[2] = X[2].replace('4','')\n",
    "# X[2] = X[2].replace('5','')\n",
    "# X[2] = X[2].replace('6','')\n",
    "# X[2] = X[2].replace('7','')\n",
    "# X[2] = X[2].replace('8','')\n",
    "# X[2] = X[2].replace('9','')\n",
    "\n",
    "# X[2] = X[2].replace('<ul>','')\n",
    "# X[2] = X[2].replace('<li>','')\n",
    "# X[2] = X[2].replace('<div>','')\n",
    "\n",
    "# X[2] = X[2].replace('</ul>','')\n",
    "# X[2] = X[2].replace('</li>','')\n",
    "# X[2] = X[2].replace('</div>','')\n",
    "\n",
    "# X[2] = X[2].replace('<','')\n",
    "# X[2] = X[2].replace('<','')\n",
    "# X[2] = X[2].replace('<','')\n",
    "\n",
    "# X[2] = X[2].replace('>','')\n",
    "# X[2] = X[2].replace('>','')\n",
    "# X[2] = X[2].replace('>','')\n",
    "\n",
    "# X[2] = X[2].replace('%','')\n",
    "# X[2] = X[2].replace(':','')\n",
    "# X[2] = X[2].replace(',','')\n",
    "# X[2] = X[2].replace('.','')\n",
    "# X[2] = X[2].replace('/','')\n",
    "# X[2] = X[2].replace('&','')\n",
    "# X[2] = X[2].replace(';','')\n",
    "# X[2] = X[2].replace('\"','')\n",
    "\n",
    "# X[2] = X[2].replace('$','')\n",
    "# X[2] = X[2].replace('+','')\n",
    "# X[2] = X[2].replace('=','')\n",
    "# X[2] = X[2].replace('~','')\n",
    "# # X[2] = X[2].replace('\\',' ')\n",
    "\n",
    "# # X = X.name + X.descrption\n",
    "# # X = X.str.lower()\n",
    "# X = str(X)\n",
    "# X = pd.DataFrame(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(X)\n",
    "# print(integer_encoded)\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "onehot_encoded = pd.DataFrame(onehot_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "integer_encoded = pd.DataFrame(integer_encoded)\n",
    "# integer_encoded = pd.get_dummies(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn import datasets\n",
    "# from sklearn import metrics\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# %pylab inline \n",
    "# %matplotlib inline\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test = train_test_split( integer_encoded, test_size = 0.5)\n",
    "\n",
    "# y_train = train_labels\n",
    "# y_train.set_index('id',inplace=True)\n",
    "# y_train.index.rename('id', inplace=True)\n",
    "\n",
    "# y_test = pd.read_csv(\"/Users/eashan22/Desktop/DataMining/FinalProject/Dataset/submission.csv\")\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# model = LogisticRegression()\n",
    "# model.fit(X_train,y_train)\n",
    "# pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "#                     ('pca', PCA(n_components=1)),\n",
    "#                     ('clf', LogisticRegression())])\n",
    "# pipe_lr.fit(X_train, y_train)\n",
    "# y_pred = model.predict_proba(X_test)\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "# param_grid = [{'clf__C': param_range, }]\n",
    "\n",
    "# gs = GridSearchCV(estimator=pipe_lr, \n",
    "#                   param_grid=param_grid, \n",
    "#                   scoring='accuracy', \n",
    "#                   cv=14,\n",
    "#                   n_jobs=-1)\n",
    "\n",
    "# gs = gs.fit(X_train, y_train)\n",
    "# # print(gs.best_score_)\n",
    "# # print(gs.best_params_)\n",
    "\n",
    "# clf = gs.best_estimator_\n",
    "# clf.fit(X_train, y_train)\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# gs = GridSearchCV(estimator=pipe_lr,\n",
    "#                   param_grid=param_grid,\n",
    "#                   scoring='accuracy',\n",
    "#                   cv=13)\n",
    "\n",
    "# # Note: Optionally, you could use cv=2 \n",
    "# # in the GridSearchCV above to produce\n",
    "# # the 5 x 2 nested CV that is shown in the figure.\n",
    "\n",
    "# scores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=14)\n",
    "\n",
    "# Y_pred = pd.DataFrame(y_pred, columns=[0,'score'])\n",
    "# Y_pred.index = Y_pred.index+18142\n",
    "# Y_pred = Y_pred.drop(0,axis=1)\n",
    "# result = pd.concat([y_test['id'],Y_pred], axis=1)\n",
    "# Y_pred.index.rename('id', inplace=True)\n",
    "# Y_pred\n",
    "# #Y_pred.to_csv('/Users/eashan22/Desktop/DataMining/FinalProject/Dataset/results/RF_results19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18141,)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = train_test_split(X, test_size = 0.50)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52351 unique tokens.\n",
      "[8427, 863, 1777, 134, 7365, 9110, 1234, 121, 192, 22, 2755, 333, 2, 1, 13, 25200, 1, 1, 37, 25201, 1, 1, 25202, 447, 17, 3763, 14, 25203, 185, 17, 46, 1108, 1, 1, 25204, 25205, 18141, 25206, 25207, 25208, 25209, 239, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS=200000\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                      lower=False)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "# sequences_valid=tokenizer.texts_to_sequences(val_data)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "print(sequences_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train tensor: (18141, 6)\n",
      "Shape of label train tensor: (18141, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(sequences_train, maxlen=6)\n",
    "# X_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1])\n",
    "# y_train = pad_sequences(sequences_train, maxlen=6)\n",
    "y_train = to_categorical(np.asarray(train_labels.score))\n",
    "# y_val = to_categorical(np.asarray(train_labels[val_data['id']]))\n",
    "\n",
    "print('Shape of X train tensor:', X_train.shape)#,X_val.shape)\n",
    "print('Shape of label train tensor:', y_train.shape)#,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('/Users/eashan22/Desktop/DataMining/FinalProject/Dataset/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "EMBEDDING_DIM=300\n",
    "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "EMBEDDING_DIM=300\n",
    "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
    "\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"reshape_4/Reshape:0\", shape=(?, 300), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "sequence_length = X_train.shape[1]\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 100\n",
    "drop = 0.5\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=2, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs, output)\n",
    "print(reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "18141/18141 [==============================] - 63s 3ms/step - loss: 0.6454 - acc: 0.6875\n",
      "Epoch 2/3\n",
      "18141/18141 [==============================] - 64s 4ms/step - loss: 0.6287 - acc: 0.6882\n",
      "Epoch 3/3\n",
      "18141/18141 [==============================] - 62s 3ms/step - loss: 0.6090 - acc: 0.6897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x140748dd8>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = Adam(lr=1e-3)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])\n",
    "#callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "model.fit(X_train, y_train, batch_size=50, epochs=3, verbose=1)#, callbacks=callbacks)  # starts training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "input must be a dictionary",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-221-2b3093b2a046>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     \u001b[0mfeed_handles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m       \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_dict_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msubfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfeed_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_feed_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mflatten_dict_items\u001b[0;34m(dictionary)\u001b[0m\n\u001b[1;32m    218\u001b[0m   \"\"\"\n\u001b[1;32m    219\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input must be a dictionary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m   \u001b[0mflat_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: input must be a dictionary"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(reshape, feed_dict = {reshape: np.reshape(label, (3,19))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test=tokenizer.texts_to_sequences(test_data.name)\n",
    "X_test = pad_sequences(sequences_test,maxlen=X_train.shape[1])\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18142, 2)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18142</th>\n",
       "      <td>0.376291</td>\n",
       "      <td>0.623709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18143</th>\n",
       "      <td>0.361025</td>\n",
       "      <td>0.638975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18144</th>\n",
       "      <td>0.276033</td>\n",
       "      <td>0.723967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18145</th>\n",
       "      <td>0.279643</td>\n",
       "      <td>0.720357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18146</th>\n",
       "      <td>0.159302</td>\n",
       "      <td>0.840698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18147</th>\n",
       "      <td>0.201571</td>\n",
       "      <td>0.798429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18148</th>\n",
       "      <td>0.235619</td>\n",
       "      <td>0.764381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18149</th>\n",
       "      <td>0.193847</td>\n",
       "      <td>0.806153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18150</th>\n",
       "      <td>0.199889</td>\n",
       "      <td>0.800111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18151</th>\n",
       "      <td>0.180097</td>\n",
       "      <td>0.819903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18152</th>\n",
       "      <td>0.241189</td>\n",
       "      <td>0.758811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18153</th>\n",
       "      <td>0.368378</td>\n",
       "      <td>0.631622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18154</th>\n",
       "      <td>0.239427</td>\n",
       "      <td>0.760573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18155</th>\n",
       "      <td>0.356642</td>\n",
       "      <td>0.643358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18156</th>\n",
       "      <td>0.153489</td>\n",
       "      <td>0.846511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18157</th>\n",
       "      <td>0.171373</td>\n",
       "      <td>0.828627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18158</th>\n",
       "      <td>0.157294</td>\n",
       "      <td>0.842706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18159</th>\n",
       "      <td>0.150911</td>\n",
       "      <td>0.849089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18160</th>\n",
       "      <td>0.227372</td>\n",
       "      <td>0.772628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18161</th>\n",
       "      <td>0.271417</td>\n",
       "      <td>0.728583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18162</th>\n",
       "      <td>0.259174</td>\n",
       "      <td>0.740826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18163</th>\n",
       "      <td>0.162924</td>\n",
       "      <td>0.837076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18164</th>\n",
       "      <td>0.151557</td>\n",
       "      <td>0.848443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18165</th>\n",
       "      <td>0.179645</td>\n",
       "      <td>0.820355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18166</th>\n",
       "      <td>0.242436</td>\n",
       "      <td>0.757564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18167</th>\n",
       "      <td>0.233399</td>\n",
       "      <td>0.766602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18168</th>\n",
       "      <td>0.215868</td>\n",
       "      <td>0.784132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18169</th>\n",
       "      <td>0.388071</td>\n",
       "      <td>0.611929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18170</th>\n",
       "      <td>0.182948</td>\n",
       "      <td>0.817052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18171</th>\n",
       "      <td>0.181169</td>\n",
       "      <td>0.818831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36254</th>\n",
       "      <td>0.249603</td>\n",
       "      <td>0.750397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36255</th>\n",
       "      <td>0.247001</td>\n",
       "      <td>0.752999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36256</th>\n",
       "      <td>0.343139</td>\n",
       "      <td>0.656861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36257</th>\n",
       "      <td>0.188146</td>\n",
       "      <td>0.811854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36258</th>\n",
       "      <td>0.338883</td>\n",
       "      <td>0.661117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36259</th>\n",
       "      <td>0.182956</td>\n",
       "      <td>0.817044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36260</th>\n",
       "      <td>0.125079</td>\n",
       "      <td>0.874921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36261</th>\n",
       "      <td>0.140469</td>\n",
       "      <td>0.859531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36262</th>\n",
       "      <td>0.240114</td>\n",
       "      <td>0.759886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36263</th>\n",
       "      <td>0.252724</td>\n",
       "      <td>0.747276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36264</th>\n",
       "      <td>0.184721</td>\n",
       "      <td>0.815279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36265</th>\n",
       "      <td>0.237996</td>\n",
       "      <td>0.762004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36266</th>\n",
       "      <td>0.261377</td>\n",
       "      <td>0.738623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36267</th>\n",
       "      <td>0.286880</td>\n",
       "      <td>0.713120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36268</th>\n",
       "      <td>0.159801</td>\n",
       "      <td>0.840199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36269</th>\n",
       "      <td>0.234808</td>\n",
       "      <td>0.765192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36270</th>\n",
       "      <td>0.210762</td>\n",
       "      <td>0.789238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36271</th>\n",
       "      <td>0.144224</td>\n",
       "      <td>0.855776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36272</th>\n",
       "      <td>0.321813</td>\n",
       "      <td>0.678187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36273</th>\n",
       "      <td>0.235769</td>\n",
       "      <td>0.764231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36274</th>\n",
       "      <td>0.249916</td>\n",
       "      <td>0.750084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36275</th>\n",
       "      <td>0.239074</td>\n",
       "      <td>0.760926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36276</th>\n",
       "      <td>0.157591</td>\n",
       "      <td>0.842409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36277</th>\n",
       "      <td>0.450470</td>\n",
       "      <td>0.549530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36278</th>\n",
       "      <td>0.277251</td>\n",
       "      <td>0.722749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36279</th>\n",
       "      <td>0.251470</td>\n",
       "      <td>0.748530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36280</th>\n",
       "      <td>0.259648</td>\n",
       "      <td>0.740352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36281</th>\n",
       "      <td>0.176333</td>\n",
       "      <td>0.823667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36282</th>\n",
       "      <td>0.154735</td>\n",
       "      <td>0.845265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36283</th>\n",
       "      <td>0.253380</td>\n",
       "      <td>0.746620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18142 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "id                       \n",
       "18142  0.376291  0.623709\n",
       "18143  0.361025  0.638975\n",
       "18144  0.276033  0.723967\n",
       "18145  0.279643  0.720357\n",
       "18146  0.159302  0.840698\n",
       "18147  0.201571  0.798429\n",
       "18148  0.235619  0.764381\n",
       "18149  0.193847  0.806153\n",
       "18150  0.199889  0.800111\n",
       "18151  0.180097  0.819903\n",
       "18152  0.241189  0.758811\n",
       "18153  0.368378  0.631622\n",
       "18154  0.239427  0.760573\n",
       "18155  0.356642  0.643358\n",
       "18156  0.153489  0.846511\n",
       "18157  0.171373  0.828627\n",
       "18158  0.157294  0.842706\n",
       "18159  0.150911  0.849089\n",
       "18160  0.227372  0.772628\n",
       "18161  0.271417  0.728583\n",
       "18162  0.259174  0.740826\n",
       "18163  0.162924  0.837076\n",
       "18164  0.151557  0.848443\n",
       "18165  0.179645  0.820355\n",
       "18166  0.242436  0.757564\n",
       "18167  0.233399  0.766602\n",
       "18168  0.215868  0.784132\n",
       "18169  0.388071  0.611929\n",
       "18170  0.182948  0.817052\n",
       "18171  0.181169  0.818831\n",
       "...         ...       ...\n",
       "36254  0.249603  0.750397\n",
       "36255  0.247001  0.752999\n",
       "36256  0.343139  0.656861\n",
       "36257  0.188146  0.811854\n",
       "36258  0.338883  0.661117\n",
       "36259  0.182956  0.817044\n",
       "36260  0.125079  0.874921\n",
       "36261  0.140469  0.859531\n",
       "36262  0.240114  0.759886\n",
       "36263  0.252724  0.747276\n",
       "36264  0.184721  0.815279\n",
       "36265  0.237996  0.762004\n",
       "36266  0.261377  0.738623\n",
       "36267  0.286880  0.713120\n",
       "36268  0.159801  0.840199\n",
       "36269  0.234808  0.765192\n",
       "36270  0.210762  0.789238\n",
       "36271  0.144224  0.855776\n",
       "36272  0.321813  0.678187\n",
       "36273  0.235769  0.764231\n",
       "36274  0.249916  0.750084\n",
       "36275  0.239074  0.760926\n",
       "36276  0.157591  0.842409\n",
       "36277  0.450470  0.549530\n",
       "36278  0.277251  0.722749\n",
       "36279  0.251470  0.748530\n",
       "36280  0.259648  0.740352\n",
       "36281  0.176333  0.823667\n",
       "36282  0.154735  0.845265\n",
       "36283  0.253380  0.746620\n",
       "\n",
       "[18142 rows x 2 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_submit = pd.DataFrame(index=test_data.id,data=y_pred)\n",
    "# to_submit = to_submit.drop([x for x in range(0,5)], axis=1)\n",
    "# to_submit = to_submit.rename(columns={5:'score'})\n",
    "to_submit.index.rename('id', inplace=True)\n",
    "to_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_submit.to_csv('/Users/eashan22/Desktop/DataMining/FinalProject/Dataset/keras_submit.csv')\n",
    "train_labels = train_labels.drop(train_labels.index[len(train_labels)-1])\n",
    "to_submit = pd.read_csv('/Users/eashan22/Desktop/DataMining/FinalProject/Dataset/keras_submit.csv')\n",
    "\n",
    "train_labels = train_labels.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf_RF = RandomForestRegressor(n_estimators = 100, oob_score =True, n_jobs = -1,random_state =10,\n",
    "                                max_features = \"auto\", min_samples_leaf = 50)\n",
    "clf_RF.fit(to_submit, y_train1)\n",
    "y_pred = clf_RF.predict(X_test)\n",
    "y_pred = (y_pred - y_pred .min()) / (y_pred .max() - y_pred .min())\n",
    "y_pred = pd.Series(y_pred, name='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "array = {'Samln','Huawei','Phone','Phone',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Counter(array).most_common(10)\n",
    "('Phone','2')\n",
    "('','')\n",
    "b = b.append(b,a[0])\n",
    "array_Name = data['name'].str.split(' ')\n",
    "\n",
    "\n",
    "def func(x,word)\n",
    "    for items in splite(' ',x):\n",
    "        if word == items:\n",
    "            return 1\n",
    "    return 0  \n",
    "\n",
    "for word in b:\n",
    "    data[word]=data['name'].apply(lambda:x,func(x,word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
